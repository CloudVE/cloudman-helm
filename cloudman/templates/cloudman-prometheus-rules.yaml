apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ template "cloudman.fullname" . }}-prom-rules
  namespace: {{ .Release.Namespace }}
  labels:
    app: {{ template "cloudman.name" . }}
    chart: {{ template "cloudman.chart" . }}
    release: {{ .Release.Name | quote }}
    heritage: {{ .Release.Service | quote }}
spec:
  groups:
  - name: cloudman-scaling
    rules:
    - alert: NodeUnderUtilized
      annotations:
        message: A node is underutilizing resources, and resources can be redistributed
      expr: |-
        # Pick the top 5 in each group because if several node labels have the same idle value, the sort isn't stable.
        # If the label changes, the prometheus alert stays stuck in pending.
        topk by (label_usegalaxy_org_cm_autoscaling_group) (5,
          (
              (
                # Sort available nodes in each scaling group by the amount of free CPU
                (
                sum(kube_node_status_allocatable{resource="cpu",unit="core"}) by (node) - sum(kube_pod_container_resource_requests{resource="cpu",unit="core"}) by (node)
                # Add the autoscaling group label to each node
                + on(node) group_left(label_usegalaxy_org_cm_autoscaling_group) (kube_node_labels{label_usegalaxy_org_cm_autoscaling_group!=""} * 0)
                # Check whether the node using the least CPU in each group would fit into an available node
                - ignoring(node) group_left min(
                    sum(kube_pod_container_resource_requests{resource="cpu",unit="core"}) by (node)
                    # Add the autoscaling group label to each node
                    + on(node) group_left(label_usegalaxy_org_cm_autoscaling_group) (kube_node_labels{label_usegalaxy_org_cm_autoscaling_group!=""} * 0)
                  ) by (label_usegalaxy_org_cm_autoscaling_group)
                )
                # filter out nodes which don't fit
                > 0
              )
              # CPU is worth 4x the cost of memory
              * 4
              * on(node, label_usegalaxy_org_cm_autoscaling_group)
              (
                # Sort available nodes in each scaling group by the amount of free RAM
                (
                sum(kube_node_status_allocatable{resource="memory",unit="byte"}) by (node) - sum(kube_pod_container_resource_requests{resource="memory",unit="byte"}) by (node)
                # Add the autoscaling group label to each node
                + on(node) group_left(label_usegalaxy_org_cm_autoscaling_group) (kube_node_labels{label_usegalaxy_org_cm_autoscaling_group!=""} * 0)
                # Check whether the node using the least RAM in each group would fit into an available node
                - ignoring(node) group_left min(
                    sum(kube_pod_container_resource_requests{resource="memory",unit="byte"}) by (node)
                    # Add the autoscaling group label to each node
                    + on(node) group_left(label_usegalaxy_org_cm_autoscaling_group) (kube_node_labels{label_usegalaxy_org_cm_autoscaling_group!=""} * 0)
                  ) by (label_usegalaxy_org_cm_autoscaling_group)
                ) > 0
              )
              # convert to gigabytes before joining
              / 1073741824
          )
        )
        # Add the cm node name label, ignoring nodes which do not have a name
        + on(node,label_usegalaxy_org_cm_autoscaling_group) group_left(label_usegalaxy_org_cm_node_name) (kube_node_labels{label_usegalaxy_org_cm_node_name!=""} * 0)
        # don't scale down if this autoscaling group has unschedulable nodes
        unless on(label_usegalaxy_org_cm_autoscaling_group) (kube_pod_labels{label_usegalaxy_org_cm_autoscaling_group!=""} and on(pod) kube_pod_status_unschedulable)
      for: 5m
      labels:
        severity: warning
    - alert: PodNotSchedulable
      annotations:
        cpus: '{{ "{{" }} $labels.cpus {{ "}}" }}'
        memory: '{{ "{{" }} $labels.memory {{ "}}" }}'
        message: Cluster has unschedulable pods due to insufficient CPU or memory
      expr: |-
        # Find highest cpu*memory request for each scaling group
        topk by (label_usegalaxy_org_cm_autoscaling_group) (5,
          # Find CPU requests
          (sum(kube_pod_container_resource_requests{resource="cpu",unit="core"}) by (pod)
          # Add CPUs as a label
          * on(pod) group_left(cpus) count_values("cpus", sum(kube_pod_container_resource_requests{resource="cpu",unit="core"}) by (pod))  by (pod)
          # Only filter on unschedulable pods
          and on(pod) kube_pod_status_unschedulable)
          # Match with memory requests
          * on (pod) group_left(memory)
          # Find memory requests
          (sum(kube_pod_container_resource_requests{resource="memory",unit="byte"}) by (pod)
          # Add memory as a label
          * on(pod) group_left(memory) count_values("memory", sum(kube_pod_container_resource_requests{resource="memory",unit="byte"}) by (pod)) by (pod)
          # Only filter on unschedulable pods
          and on(pod) kube_pod_status_unschedulable)
          # Add scaling group as a label
          + on(pod) group_left(label_usegalaxy_org_cm_autoscaling_group) (kube_pod_labels{label_usegalaxy_org_cm_autoscaling_group!=""} * 0)
        )
      for: 1m
      labels:
        severity: warning
    - alert: AnticipatoryScaling
      annotations:
        message: Anticipatorily schedule a node since a user is currently active
      expr: |-
        label_replace(
          rate(nginx_ingress_controller_bytes_sent_count{ingress=~".*-activity-canary",status="200"}[5m]),
          "label_usegalaxy_org_cm_autoscaling_group", "anticipatory", "", "") > 0
      for: 2s
      labels:
        severity: info
