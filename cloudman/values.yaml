# CloudMan has the same dependencies and structure as cloudlaunch,
# and only differs in the container images.

admin_firstname: "CloudMan"
admin_lastname: "Admin"
admin_email: "cloudman@someplace.org"

projman_config:
  projects:

helmsman_config:
  repositories:
  template_registries:

# CloudMan bootstrap data
cm_initial_cluster_data:

# Special global values which are accessible from all charts
global:
  domain: 192.168.99.100
  keycloak:
    namespace: keycloak
    cloudmanClient:
      realmSelector: "gvl-sso"
      clientId: "cloudman"
      realm: "gvl"

# This feature can be used to force the creation of OIDC clients
# with the cloudman deployment. However, projman-helm will now handles
# the client creation for the standard project isolation
oidc_clients: {}
  # galaxy:
  #   client_id: 'galaxy-auth'
  #   # The client secret is required if the client is not public
  #   public_client: false
  #   #client_secret: 'cf84xfps-npm5-hvmb-ntui4grybqrh'
  #   redirect_uris:
  #     - '{{ include "cloudman.root_url" . }}/*'
  # terminalman:
  #   client_id: 'terminalman'
  #   #client_secret:
  #   redirect_uris:
  #     - '{{ include "cloudman.root_url" . }}/*'
  # jupyterhub:
  #   client_id: 'jupyterhub'
  #   #client_secret:
  #   redirect_uris:
  #     - '{{ include "cloudman.root_url" . }}/*'
  # nextcloud:
  #   client_id: 'nextcloud'
  #   #client_secret:
  #   redirect_uris:
  #     - '{{ include "cloudman.root_url" . }}/*'

cloudlaunch:
  nameOverride: cloudman
  container_name: cloudman-ui
  image:
    repository: cloudve/cloudman-ui
    tag: latest

  ingress:
    enabled: true
    annotations: {}
      # kubernetes.io/ingress.class: nginx
    path: /
    hosts:
      - ~
    tls: []

  cloudlaunchserver:
    nameOverride: cloudman
    container_name: cloudman-server
    rbac:
      serviceAccount: '{{ include "cloudman.fullname" . }}-cm-svc-account'
    image:
      repository: cloudve/cloudman-server
      tag: latest
    env_prefix: CLOUDMAN
    celery_app_name: cloudman
    django_settings_module: cloudman.settings
    django_dir: /app/cloudman
    initial_data: []
    extra_init_scripts:
      load_bootstrap.sh: |
        #!/bin/sh
        sh /app/secrets/bootstrap.sh
    extra_config_mounts:
      - name: cloudman-extra-config
        mount_path: /opt/cloudman/
        config_name: '{{ template "cloudman.fullname" . }}-cm-init-config'
        read_only: true
    extra_secret_mounts:
      - name: cloudman-bootstrap
        mount_path: /app/secrets
        secret_name: '{{ template "cloudman.fullname" . }}-secret'
        read_only: true

    # TODO: include release name in configmap name
    extraVolumes: |
      - name: kubeconfig
        emptyDir: {}
    extraVolumeMounts: |
      - name: kubeconfig
        mountPath: /home/cloudman
    extra_env:
      - name: OIDC_ENABLED
        value: "True"
      - name: OIDC_METADATA_URI
        value: '{{.Values.ingress.protocol}}://auth.{{.Values.global.domain | default (index .Values.ingress.hosts 0)}}/auth/realms/{{ .Values.global.keycloak.cloudmanClient.realm }}/.well-known/openid-configuration'
      - name: OIDC_CLIENT_ID
        value: "cloudman"
      - name: OIDC_PUBLIC_URI
        value: "{{.Values.ingress.protocol}}://{{.Values.global.domain | default (index .Values.ingress.hosts 0)}}/cloudman"
    postgresql:
      enabled: true
      postgresqlDatabase: cloudman
      postgresqlUsername: cloudman
      # The cloudman user password will be autogenerated by cloudlaunchserver chart if not specified
      # postgresqlPassword: some_pass
      # The postgres superuser password will not be set if not specified. If you need a postgres superuser,
      # this should be set at launch (eg: for access to manually modify databases on the running server)
      # postgresqlPostgresPassword: admin_pass
    ingress:
      enabled: true
      path: /cloudman
      hosts:
        - ~
      protocol: https

kubeprometheus:
  grafana:
    # adminPassword: "changeme"
    sidecar:
      dashboards:
        searchNamespace: "ALL"
    grafana.ini:
      server:
        root_url: "https://{{ .Values.global.domain }}/grafana"
        serve_from_sub_path: true
      auth.generic_oauth:
        enabled: true
        client_id: "cloudman"
        auth_url: "https://auth.{{ .Values.global.domain }}/auth/realms/{{ .Values.global.keycloak.cloudmanClient.realm }}/protocol/openid-connect/auth"
        token_url: "https://auth.{{ .Values.global.domain }}/auth/realms/{{ .Values.global.keycloak.cloudmanClient.realm }}/protocol/openid-connect/token"
        api_url: "https://auth.{{ .Values.global.domain }}/auth/realms/{{ .Values.global.keycloak.cloudmanClient.realm }}/protocol/openid-connect/userinfo"
        tls_skip_verify_insecure: true
        allow_sign_up: true
      auth:
        oauth_auto_login: true
        disable_login_form: true
        disable_signout_menu: true
      auth.anonymous:
        enabled: false
      security:
        allow_embedding: true
    ingress:
      enabled: true
      path: /grafana
      hosts: []
  prometheus:
    prometheusSpec:
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: ebs
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 50Gi
      serviceMonitorSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
      ruleSelector:
        matchLabels:
          release: "cloudman"
  alertmanager:
    tplConfig: true
    config:
      global:
        resolve_timeout: 5m
        http_config:
          basic_auth:
            username: autoscaleuser
            # password: changemetoarandomvalue
      receivers:
        - name: 'scaleup'
          webhook_configs:
            - url: http://{{ .Release.Name }}-cloudlaunchserver:8000/cloudman/api/v1/clusters/1/signals/scaleup/
              send_resolved: false
        - name: 'scaledown'
          webhook_configs:
            - url: http://{{ .Release.Name }}-cloudlaunchserver:8000/cloudman/api/v1/clusters/1/signals/scaledown/
              send_resolved: false
        - name: 'blackhole'
      route:
        group_by:
        - alertname
        group_interval: 5m
        group_wait: 30s
        receiver: blackhole
        repeat_interval: 8h
        routes:
        - group_interval: 2m
          group_wait: 5s
          match:
            alertname: NodeUnderUtilized
          receiver: scaledown
          repeat_interval: 5m
        - group_interval: 5m
          group_wait: 5s
          match:
            alertname: PodNotSchedulable
          repeat_interval: 5m
          receiver: scaleup
        - group_interval: 2m
          group_wait: 1s
          match:
            alertname: AnticipatoryScaling
          repeat_interval: 2m
          receiver: scaleup
        - match:
            severity: critical
          receiver: email
      inhibit_rules:
      - target_match:
          alertname: NodeUnderUtilized
          label_usegalaxy_org_cm_autoscaling_group: anticipatory
        source_match:
          alertname: AnticipatoryScaling
          label_usegalaxy_org_cm_autoscaling_group: anticipatory
      - target_match:
          alertname: KubeControllerManagerDown
        source_match:
          alertname: Watchdog
        equal: ['prometheus']
      - target_match:
          alertname: KubeSchedulerDown
        source_match:
          alertname: Watchdog
        equal: ['prometheus']
      - target_match:
          alertname: etcdInsufficientMembers
        source_match:
          alertname: Watchdog
        equal: ['prometheus']
      - target_match:
          alertname: etcdMembersDown
        source_match:
          alertname: Watchdog
        equal: ['prometheus']
influxdb:
  setDefaultUser:
    enabled: true
    user:
      username: "admin"
      # password: "iAmRequired"
  initScripts:
    enabled: true
  persistence:
    storageClass: ebs
